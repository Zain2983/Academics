{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "p3jo9cp_Ju3K",
        "jugw8lWBJbae",
        "JOCeKegcJk4y",
        "tvjId1EvJ0s8",
        "xHlhlE8fJ8f4"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Language Detection - DAV Project**\n",
        "\n",
        "\n",
        "**Hamza Ahmed      21L-6292**\n",
        "\n",
        "**Zain Al Abidin   21L-6260**"
      ],
      "metadata": {
        "id": "pU91PhUsK4K6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scraping"
      ],
      "metadata": {
        "id": "p3jo9cp_Ju3K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Scraping\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "def scrape_wikipedia(language_code):\n",
        "    url = f\"https://{language_code}.wikipedia.org/wiki/Main_Page\"\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.81 Safari/537.36\"\n",
        "    }\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "        articles = []\n",
        "        for heading in soup.find_all(\"p\", class_=\"\"):\n",
        "            articles.append(heading.text.strip())\n",
        "\n",
        "        # response = requests.get(url, headers=headers)\n",
        "        # response.raise_for_status()\n",
        "        # soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "        # for heading in soup.find_all(\"span\", class_=\"mw-headline\"):\n",
        "        #     articles.append(heading.text.strip())\n",
        "\n",
        "        # response = requests.get(url, headers=headers)\n",
        "        # response.raise_for_status()\n",
        "        # soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "        # for heading in soup.find_all(\"span\", class_=\"mw-headline\"):\n",
        "        #     articles.append(heading.text.strip())\n",
        "\n",
        "\n",
        "        return articles\n",
        "\n",
        "    except requests.HTTPError as errh:\n",
        "        print(f\"HTTP Error: {errh}\")\n",
        "    except requests.ConnectionError as errc:\n",
        "        print(f\"Error Connecting: {errc}\")\n",
        "    except requests.Timeout as errt:\n",
        "        print(f\"Timeout Error: {errt}\")\n",
        "    except requests.RequestException as err:\n",
        "        print(f\"Other Error: {err}\")\n",
        "    return []\n",
        "\n",
        "\n",
        "\n",
        "languages = {\n",
        "    \"tt\": \"Tatar\",\n",
        "    \"en\": \"English\",\n",
        "    \"it\": \"Italian\",\n",
        "    \"ca\": \"Catalan\",\n",
        "    \"pl\": \"Polish\",\n",
        "    \"ar\": \"Arabic\",\n",
        "    \"su\": \"Sundanese\",\n",
        "    \"he\": \"Hebrew\",\n",
        "    \"yo\": \"Yoruba\",\n",
        "    \"pa\": \"Punjabi\",\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "data = []\n",
        "for lang_code, lang_name in languages.items():\n",
        "    wiki_articles = scrape_wikipedia(lang_code)\n",
        "    if wiki_articles:\n",
        "        for article in wiki_articles:\n",
        "            data.append({\"Language\": lang_name, \"Text\": article})\n",
        "\n",
        "# for lang_code, lang_name in languages.items():\n",
        "#     wiki_articles = scrape_wikipedia(lang_code)\n",
        "#     if wiki_articles:\n",
        "#         for article in wiki_articles:\n",
        "#             data.append({\"Language\": lang_name, \"Text\": article})\n",
        "\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "df.dropna(inplace=True)\n",
        "df.to_csv(\"multilingual_wikipedia_dataset.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "bCKBTzlhW79o"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Manual Entry of English // Please ignore this part"
      ],
      "metadata": {
        "id": "jugw8lWBJbae"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "english_data1 = \"\"\"As we have seen in the movies that the hero is very popular in his locality so do my father because he is ready to help everyone at any time. I remember a recent incident when one of his\n",
        "colleagues and friend fall ill and needed help then my father wholeheartedly supported him and his family. My father helped them in the most critical times when they needed help. In addition, he also\n",
        "arranged for finances for them. As at that time they are not financially stable and his colleagues are the only earning member of the family. Also, he spends an hour in the hospital to complete the\n",
        "formalities and arranging the medicine. Besides, my father many of his other colleagues also contributed money and other means to help the family of ill. After the complete recovery of the colleague he\n",
        "and his family visited our home and especially thanked my father for his efforts and help. After that incident whenever I think of that incident I feel proud of him. I learned this willingness to help\n",
        "others from him. My father has other qualities too he also hard working and punctual and dedicated to his work and often work late hours. But for this, he never compromises his time with family. Besides,\n",
        "in the office and among our relatives everyone appreciates him for his hard work. \"\"\"\n",
        "\n",
        "english_data2 = \"\"\"Also, during his college and school day is was among the top performer and won many awards. In his office often announced\n",
        "as the employee of the month. Besides, there were times when I have seen him working late at hours even on holidays. He knows how to maintain his professional and personal life and that’s the thing I love\n",
        "most about him. Many of my friend’s complaint that their father does not look after them because of their work. Also, they even said that they hate their father because he often comes late and doesn’t love\n",
        "them. Besides, I am lucky that my father is different from them as he gives importance to his family first and after that work. In addition, he makes sure to come at a time and give time to us (his family).\n",
        "He is a caring husband and helps our mother in small household work to share her workload. To conclude, my father is a responsible man who clearly knows that between work and family; family comes first.\n",
        "Also, he is always ready to help others no matter what he is doing. Besides, he completely dedicated to his work but does not ignore us in the event. Above all, we stay put to him as we have learned all our\n",
        "good deeds from him.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "hy6oz9ZlCiT1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.append({\"Language\": \"English\", \"Text\": english_data1})\n",
        "data.append({\"Language\": \"English\", \"Text\": english_data2})\n",
        "\n",
        "df = pd.DataFrame(data)"
      ],
      "metadata": {
        "id": "Km7XQs_cEAtV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre Processing"
      ],
      "metadata": {
        "id": "JOCeKegcJk4y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Data Wrangling\n",
        "import pandas as pd\n",
        "import pandas as pd\n",
        "import re\n",
        "from unicodedata import normalize\n",
        "\n",
        "# aggregated_df = df.groupby('Language')['Text'].agg(lambda x: ' '.join(x)).reset_index()\n",
        "aggregated_df=pd.read_csv(\"multilingual_wikipedia_dataset.csv\")\n",
        "aggregated_df.dropna(inplace=True)\n",
        "\n",
        "aggregated_df['Text'] = aggregated_df['Text'].str.lower()\n",
        "\n",
        "aggregated_df['Text'] = aggregated_df['Text'].apply(lambda x: re.findall(r'\\b\\w+\\b', x))\n",
        "\n",
        "aggregated_df['Text'] = aggregated_df['Text'].apply(lambda x: [word for word in x if word.isalnum()])\n",
        "\n",
        "# aggregated_df['Text'] = aggregated_df['Text'].apply(lambda x: [normalize('NFKD', word).encode('ASCII', 'ignore').decode('utf-8') for word in x])\n",
        "\n",
        "print(aggregated_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HAG_jYvv8RW4",
        "outputId": "b5eed7b8-57d0-4642-94dc-654ad5d39e19"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Language                                               Text\n",
            "0      Tatar  [ирекле, эчтәлекле, энциклопедияне, һәркем, яз...\n",
            "1      Tatar  [тулы, исемлек, эчтәлек, порталлар, latin, iml...\n",
            "6      Tatar  [әлҗәзаирдә, берберлар, яңа, елы, төрекмәнстан...\n",
            "9      Tatar  [манусмрти, санскрит, телендә, मन, स, म, त, шу...\n",
            "10     Tatar                                           [дәвамы]\n",
            "..       ...                                                ...\n",
            "335  Punjabi    [ਝ, ੜ, ਤ, ਲ, ਗ, ਆ, ਚ, ਨ, ਬ, ਰ, ਤਸਵ, ਰ, fir0002]\n",
            "336  Punjabi  [ਉਪ, ਸ, ਰ, ਣ, ਆ, ਦ, ਖਣ, ਲਈ, ਕ, ਰਪ, ਕਰਕ, ਤ, ਰ, ...\n",
            "347  Punjabi  [ਇਹ, ਵ, ਕ, ਪ, ਡ, ਆ, ਪ, ਜ, ਬ, ਵ, ਚ, ਲ, ਖ, ਆ, ਗ,...\n",
            "348  Punjabi  [स, स, क, त, प, ल, भ, जप, र, मर, ठ, ಕನ, ನಡ, தம...\n",
            "350  Punjabi  [ਵ, ਕ, ਪ, ਡ, ਆ, ਵ, ਲ, ਟ, ਅਰ, ਸ, ਪ, ਦਕ, ਦ, ਆਰ, ...\n",
            "\n",
            "[256 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Embedding\n",
        "row_counts = df['Language'].value_counts()\n",
        "print(row_counts.head(15))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QmDJpjar__m3",
        "outputId": "5c0d11b7-1d2d-4b4b-e783-c40a756489a5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tatar        79\n",
            "Italian      55\n",
            "Catalan      35\n",
            "Polish       34\n",
            "Arabic       33\n",
            "Sundanese    29\n",
            "Yoruba       28\n",
            "Punjabi      27\n",
            "Hebrew       24\n",
            "English       7\n",
            "Name: Language, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.Language.unique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WqcXfPi8Prd-",
        "outputId": "f79433d3-9792-47f6-a780-50e901679e3e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['Tatar', 'English', 'Italian', 'Catalan', 'Polish', 'Arabic',\n",
              "       'Sundanese', 'Hebrew', 'Yoruba', 'Punjabi'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "word2vec_model = Word2Vec(aggregated_df['Text'], vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "def average_word_embedding(sentence):\n",
        "    embeddings = [word2vec_model.wv[word] for word in sentence if word in word2vec_model.wv]\n",
        "    if embeddings:\n",
        "        return np.mean(embeddings, axis=0)\n",
        "    else:\n",
        "        return np.zeros(word2vec_model.vector_size)\n",
        "\n",
        "aggregated_df['Word_Embedding'] = aggregated_df['Text'].apply(lambda x: average_word_embedding(x))\n",
        "\n",
        "df.to_csv(\"dataset.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "3II3nxJQAXvL"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(aggregated_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_nquTBcAXrE",
        "outputId": "a0e3d5c1-1bf3-407f-fc55-5497f4de9103"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Language                                               Text  \\\n",
            "0      Tatar  [ирекле, эчтәлекле, энциклопедияне, һәркем, яз...   \n",
            "1      Tatar  [тулы, исемлек, эчтәлек, порталлар, latin, iml...   \n",
            "6      Tatar  [әлҗәзаирдә, берберлар, яңа, елы, төрекмәнстан...   \n",
            "9      Tatar  [манусмрти, санскрит, телендә, मन, स, म, त, шу...   \n",
            "10     Tatar                                           [дәвамы]   \n",
            "..       ...                                                ...   \n",
            "335  Punjabi    [ਝ, ੜ, ਤ, ਲ, ਗ, ਆ, ਚ, ਨ, ਬ, ਰ, ਤਸਵ, ਰ, fir0002]   \n",
            "336  Punjabi  [ਉਪ, ਸ, ਰ, ਣ, ਆ, ਦ, ਖਣ, ਲਈ, ਕ, ਰਪ, ਕਰਕ, ਤ, ਰ, ...   \n",
            "347  Punjabi  [ਇਹ, ਵ, ਕ, ਪ, ਡ, ਆ, ਪ, ਜ, ਬ, ਵ, ਚ, ਲ, ਖ, ਆ, ਗ,...   \n",
            "348  Punjabi  [स, स, क, त, प, ल, भ, जप, र, मर, ठ, ಕನ, ನಡ, தம...   \n",
            "350  Punjabi  [ਵ, ਕ, ਪ, ਡ, ਆ, ਵ, ਲ, ਟ, ਅਰ, ਸ, ਪ, ਦਕ, ਦ, ਆਰ, ...   \n",
            "\n",
            "                                        Word_Embedding  \n",
            "0    [0.0030369936, -0.0032465241, -0.0032837738, 0...  \n",
            "1    [0.0025273748, 0.0014053872, 0.0027626548, 0.0...  \n",
            "6    [0.0005608912, 0.001111144, 8.374608e-06, 0.00...  \n",
            "9    [0.0012508525, 0.00023462166, 0.0005194695, 0....  \n",
            "10   [0.0062039495, 0.0014124584, -0.0020156396, -0...  \n",
            "..                                                 ...  \n",
            "335  [-0.0024841847, 0.000101501966, 0.0027660953, ...  \n",
            "336  [-3.827571e-05, -0.00044822294, 0.004450599, -...  \n",
            "347  [-0.0024328143, -0.0009600905, 0.002719066, 0....  \n",
            "348  [-0.0022511221, 0.00113516, 7.9755744e-05, -0....  \n",
            "350  [-0.0014445496, -0.0016948376, 0.00323926, 0.0...  \n",
            "\n",
            "[256 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training"
      ],
      "metadata": {
        "id": "tvjId1EvJ0s8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, GlobalMaxPooling1D, Dense, Bidirectional, Dropout\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assuming 'aggregated_df' is your dataset DataFrame\n",
        "# Assuming 'text' contains the text data for each sample\n",
        "# Assuming 'Language' is the target variable\n",
        "\n",
        "# Tokenize the text and pad sequences\n",
        "max_length = 100\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(aggregated_df['Text'])\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "X = pad_sequences(tokenizer.texts_to_sequences(aggregated_df['Text']), maxlen=max_length, padding='post')\n",
        "# X = np.vstack(aggregated_df['Word_Embedding'])  # Assuming 'Word_Embedding' contains numpy arrays\n",
        "# Encode the target variable\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(aggregated_df['Language'])\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
        "\n",
        "# Define and compile the model\n",
        "embedding_dim = 100\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length, trainable=True),\n",
        "    Bidirectional(LSTM(128, return_sequences=True)),\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "\n",
        "    Dense(len(label_encoder.classes_), activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=30, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the model on test data\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "\n",
        "print(\"Test Accuracy:\", test_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MV9qhgl9ZBCQ",
        "outputId": "9b0b6891-cdcb-4890-ae7e-592677e1a31e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "8/8 [==============================] - 8s 399ms/step - loss: 2.2926 - accuracy: 0.1174 - val_loss: 2.2519 - val_accuracy: 0.2692\n",
            "Epoch 2/30\n",
            "8/8 [==============================] - 2s 283ms/step - loss: 2.2481 - accuracy: 0.1696 - val_loss: 2.1800 - val_accuracy: 0.2692\n",
            "Epoch 3/30\n",
            "8/8 [==============================] - 3s 357ms/step - loss: 2.2287 - accuracy: 0.1522 - val_loss: 2.1694 - val_accuracy: 0.2692\n",
            "Epoch 4/30\n",
            "8/8 [==============================] - 3s 419ms/step - loss: 2.2212 - accuracy: 0.1609 - val_loss: 2.1564 - val_accuracy: 0.2692\n",
            "Epoch 5/30\n",
            "8/8 [==============================] - 3s 362ms/step - loss: 2.1982 - accuracy: 0.2000 - val_loss: 2.1266 - val_accuracy: 0.3077\n",
            "Epoch 6/30\n",
            "8/8 [==============================] - 3s 319ms/step - loss: 2.1577 - accuracy: 0.2348 - val_loss: 2.1002 - val_accuracy: 0.3462\n",
            "Epoch 7/30\n",
            "8/8 [==============================] - 2s 285ms/step - loss: 2.0740 - accuracy: 0.2957 - val_loss: 2.0086 - val_accuracy: 0.3462\n",
            "Epoch 8/30\n",
            "8/8 [==============================] - 3s 335ms/step - loss: 1.9542 - accuracy: 0.2739 - val_loss: 1.9148 - val_accuracy: 0.3462\n",
            "Epoch 9/30\n",
            "8/8 [==============================] - 4s 446ms/step - loss: 1.7864 - accuracy: 0.3478 - val_loss: 1.6783 - val_accuracy: 0.3462\n",
            "Epoch 10/30\n",
            "8/8 [==============================] - 2s 272ms/step - loss: 1.6386 - accuracy: 0.4130 - val_loss: 1.6340 - val_accuracy: 0.5385\n",
            "Epoch 11/30\n",
            "8/8 [==============================] - 2s 285ms/step - loss: 1.5314 - accuracy: 0.5000 - val_loss: 1.5064 - val_accuracy: 0.6538\n",
            "Epoch 12/30\n",
            "8/8 [==============================] - 2s 283ms/step - loss: 1.3354 - accuracy: 0.6087 - val_loss: 1.3983 - val_accuracy: 0.6538\n",
            "Epoch 13/30\n",
            "8/8 [==============================] - 2s 280ms/step - loss: 1.2087 - accuracy: 0.6435 - val_loss: 1.2465 - val_accuracy: 0.6154\n",
            "Epoch 14/30\n",
            "8/8 [==============================] - 4s 506ms/step - loss: 0.9922 - accuracy: 0.6739 - val_loss: 1.1419 - val_accuracy: 0.6923\n",
            "Epoch 15/30\n",
            "8/8 [==============================] - 2s 265ms/step - loss: 0.9225 - accuracy: 0.7565 - val_loss: 1.0446 - val_accuracy: 0.6923\n",
            "Epoch 16/30\n",
            "8/8 [==============================] - 2s 278ms/step - loss: 0.7421 - accuracy: 0.7957 - val_loss: 0.9070 - val_accuracy: 0.6923\n",
            "Epoch 17/30\n",
            "8/8 [==============================] - 2s 274ms/step - loss: 0.6305 - accuracy: 0.8130 - val_loss: 0.8332 - val_accuracy: 0.7308\n",
            "Epoch 18/30\n",
            "8/8 [==============================] - 2s 289ms/step - loss: 0.5392 - accuracy: 0.8696 - val_loss: 0.8032 - val_accuracy: 0.7692\n",
            "Epoch 19/30\n",
            "8/8 [==============================] - 3s 366ms/step - loss: 0.4650 - accuracy: 0.8739 - val_loss: 0.7723 - val_accuracy: 0.7308\n",
            "Epoch 20/30\n",
            "8/8 [==============================] - 3s 420ms/step - loss: 0.3644 - accuracy: 0.9130 - val_loss: 0.7464 - val_accuracy: 0.7308\n",
            "Epoch 21/30\n",
            "8/8 [==============================] - 2s 269ms/step - loss: 0.3867 - accuracy: 0.8957 - val_loss: 0.7509 - val_accuracy: 0.7308\n",
            "Epoch 22/30\n",
            "8/8 [==============================] - 2s 283ms/step - loss: 0.3278 - accuracy: 0.9261 - val_loss: 0.6976 - val_accuracy: 0.7692\n",
            "Epoch 23/30\n",
            "8/8 [==============================] - 2s 270ms/step - loss: 0.2582 - accuracy: 0.9391 - val_loss: 0.7370 - val_accuracy: 0.8462\n",
            "Epoch 24/30\n",
            "8/8 [==============================] - 2s 275ms/step - loss: 0.2958 - accuracy: 0.9130 - val_loss: 0.7559 - val_accuracy: 0.8462\n",
            "Epoch 25/30\n",
            "8/8 [==============================] - 4s 500ms/step - loss: 0.2189 - accuracy: 0.9522 - val_loss: 0.7075 - val_accuracy: 0.8462\n",
            "Epoch 26/30\n",
            "8/8 [==============================] - 2s 280ms/step - loss: 0.1869 - accuracy: 0.9435 - val_loss: 0.7038 - val_accuracy: 0.8462\n",
            "Epoch 27/30\n",
            "8/8 [==============================] - 2s 279ms/step - loss: 0.1627 - accuracy: 0.9478 - val_loss: 0.6702 - val_accuracy: 0.8077\n",
            "Epoch 28/30\n",
            "8/8 [==============================] - 2s 269ms/step - loss: 0.1396 - accuracy: 0.9870 - val_loss: 0.8684 - val_accuracy: 0.8077\n",
            "Epoch 29/30\n",
            "8/8 [==============================] - 2s 269ms/step - loss: 0.1300 - accuracy: 0.9826 - val_loss: 0.7112 - val_accuracy: 0.8077\n",
            "Epoch 30/30\n",
            "8/8 [==============================] - 3s 357ms/step - loss: 0.0986 - accuracy: 0.9913 - val_loss: 0.7209 - val_accuracy: 0.8462\n",
            "1/1 [==============================] - 0s 88ms/step - loss: 0.7209 - accuracy: 0.8462\n",
            "Test Accuracy: 0.8461538553237915\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(aggregated_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qRQM6FNpckut",
        "outputId": "5c05cf6e-7931-4c2f-f3cf-0da44cdb7fc5"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Language                                               Text  \\\n",
            "0      Tatar  [ирекле, эчтәлекле, энциклопедияне, һәркем, яз...   \n",
            "1      Tatar  [тулы, исемлек, эчтәлек, порталлар, latin, iml...   \n",
            "6      Tatar  [әлҗәзаирдә, берберлар, яңа, елы, төрекмәнстан...   \n",
            "9      Tatar  [манусмрти, санскрит, телендә, मन, स, म, त, шу...   \n",
            "10     Tatar                                           [дәвамы]   \n",
            "..       ...                                                ...   \n",
            "335  Punjabi    [ਝ, ੜ, ਤ, ਲ, ਗ, ਆ, ਚ, ਨ, ਬ, ਰ, ਤਸਵ, ਰ, fir0002]   \n",
            "336  Punjabi  [ਉਪ, ਸ, ਰ, ਣ, ਆ, ਦ, ਖਣ, ਲਈ, ਕ, ਰਪ, ਕਰਕ, ਤ, ਰ, ...   \n",
            "347  Punjabi  [ਇਹ, ਵ, ਕ, ਪ, ਡ, ਆ, ਪ, ਜ, ਬ, ਵ, ਚ, ਲ, ਖ, ਆ, ਗ,...   \n",
            "348  Punjabi  [स, स, क, त, प, ल, भ, जप, र, मर, ठ, ಕನ, ನಡ, தம...   \n",
            "350  Punjabi  [ਵ, ਕ, ਪ, ਡ, ਆ, ਵ, ਲ, ਟ, ਅਰ, ਸ, ਪ, ਦਕ, ਦ, ਆਰ, ...   \n",
            "\n",
            "                                        Word_Embedding  \n",
            "0    [0.0030369936, -0.0032465241, -0.0032837738, 0...  \n",
            "1    [0.0025273748, 0.0014053872, 0.0027626548, 0.0...  \n",
            "6    [0.0005608912, 0.001111144, 8.374608e-06, 0.00...  \n",
            "9    [0.0012508525, 0.00023462166, 0.0005194695, 0....  \n",
            "10   [0.0062039495, 0.0014124584, -0.0020156396, -0...  \n",
            "..                                                 ...  \n",
            "335  [-0.0024841847, 0.000101501966, 0.0027660953, ...  \n",
            "336  [-3.827571e-05, -0.00044822294, 0.004450599, -...  \n",
            "347  [-0.0024328143, -0.0009600905, 0.002719066, 0....  \n",
            "348  [-0.0022511221, 0.00113516, 7.9755744e-05, -0....  \n",
            "350  [-0.0014445496, -0.0016948376, 0.00323926, 0.0...  \n",
            "\n",
            "[256 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Generate predictions on test data\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)  # Get the predicted classes\n",
        "\n",
        "# Convert encoded labels back to original labelsma\n",
        "y_test_original = label_encoder.inverse_transform(y_test)\n",
        "y_pred_original = label_encoder.inverse_transform(y_pred_classes)\n",
        "\n",
        "# Generate classification report\n",
        "print(classification_report(y_test_original, y_pred_original))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VhOqDhhkdyUq",
        "outputId": "f14250fc-cbc0-4745-b96b-8f19c7f7432b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 3s 3s/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Arabic       1.00      1.00      1.00         2\n",
            "     Catalan       1.00      1.00      1.00         1\n",
            "      Hebrew       1.00      1.00      1.00         2\n",
            "     Italian       1.00      1.00      1.00         2\n",
            "      Polish       0.50      0.75      0.60         4\n",
            "     Punjabi       0.50      1.00      0.67         1\n",
            "   Sundanese       1.00      0.50      0.67         2\n",
            "       Tatar       0.83      0.71      0.77         7\n",
            "      Yoruba       1.00      0.80      0.89         5\n",
            "\n",
            "    accuracy                           0.81        26\n",
            "   macro avg       0.87      0.86      0.84        26\n",
            "weighted avg       0.86      0.81      0.82        26\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing"
      ],
      "metadata": {
        "id": "xHlhlE8fJ8f4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'text_to_predict' is the text you want to predict the language for\n",
        "text_to_predict = input(\"Please enter yout text : \")\n",
        "\n",
        "# Preprocess the text\n",
        "sequence = tokenizer.texts_to_sequences([text_to_predict])\n",
        "padded_sequence = pad_sequences(sequence, maxlen=max_length, padding='post')\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.predict(padded_sequence)\n",
        "predicted_language_index = np.argmax(predictions)\n",
        "predicted_language = label_encoder.classes_[predicted_language_index]\n",
        "\n",
        "# Print the predicted language\n",
        "print(\"Predicted Language:\", predicted_language)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4LAnSzRfHGjv",
        "outputId": "3deaad60-0e0c-4582-caaa-3ca02f122040"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please enter yout text :  ਗੈਰ-ਮੁਨਾ\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "Predicted Language: Sundanese\n"
          ]
        }
      ]
    }
  ]
}